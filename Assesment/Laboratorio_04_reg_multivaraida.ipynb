{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiRIfl4W2cLM"
   },
   "source": [
    "# Regresion multivariada\n",
    "\n",
    "Supongamos que tenemos un conjunto de caracteristicas $X = X_1,X_2...X_j...X_n$ para realizar una  predicción $y$ con valores esperados $\\hat{y}$.  \n",
    "\n",
    "Cada X, puede ser escrito como:\n",
    " $X_1 = x_1^{(1)},x_1^{(2)}, x_1^{(3)}...x_1^{(m)}$, \n",
    "\n",
    " $X_2 = x_2^{(1)},x_2^{(2)}, x_2^{(3)}...x_2^{(m)}$, \n",
    " \n",
    " .\n",
    " \n",
    " .\n",
    " \n",
    " .\n",
    " \n",
    " $X_n = x_n^{(1)},x_n^{(2)}, x_n^{(3)}...x_n^{(m)}$. \n",
    " \n",
    "\n",
    "Siendo n el número de caracteristicas y m el número de datos de datos, \n",
    "$\\hat{y} = \\hat{y}_1^{(1)}, \\hat{y}_1^{(2)}...\\hat{y}_1^{(m)} $, el conjunto de datos etiquetados  y $y = y_1^{(1)}, y_1^{(2)}...y_1^{(m)} $ los valores predichos por un modelo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Lo anterior puede ser resumido  como:\n",
    "\n",
    "\n",
    "\n",
    "|Training|$\\hat{y}$      | X_1  | X_2  |  .  | .|. |. | X_n|\n",
    "|--------|-------|------|------|-----|--|--|--|----|\n",
    "|1|$\\hat{y}_1^{1}$ | $x_1^{1}$|$x_2^{1}$| .  | .|. |. | $x_n^{1}$|\n",
    "|2|$\\hat{y}_1^{2}$ | $x_1^{2}$|$x_2^{2}$| .  | .|. |. | $x_n^{2}$|\n",
    "|.|.         | .        |.| .  | .|. |. | |\n",
    "|.|.         | .        |.| .  | .|. |. | |\n",
    "|.|.         | .        |.| .  | .|. |. | |\n",
    "|m|$\\hat{y}_1^{m}$ | $x_1^{m}$  |$x_2^{m}$| .  | .|. |. | $x_n^{m}$|\n",
    "\n",
    "\n",
    "y el el modelo puede ser ajustado como sigue: \n",
    "\n",
    "Para un solo conjunto de datos de entrenamiento tentemos que:\n",
    "\n",
    "$y = h(\\theta_0,\\theta_1,\\theta_2,...,\\theta_n ) = \\theta_0 + \\theta_1 x_1+\\theta_2 x_2 + \\theta_3 x_3 +...+ \\theta_n x_n $.\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\Theta}(x) = [\\theta_0,\\theta_1,...,\\theta_n ]\\begin{bmatrix}\n",
    "1^{(1)}\\\\\n",
    "x_1^{(1)}\\\\\n",
    "x_2^{(1)}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x_n^{(1)}\\\\\n",
    "\\end{bmatrix} = \\Theta^T X^{(1)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Para todo el conjunto de datos, tenemos que:\n",
    "\n",
    "Sea $\\Theta^T = [\\theta_0,\\theta_1,\\theta_2,...,\\theta_n]$ una matrix $1 \\times (n+1)$ y  \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1& 1 & 1 & .&.&.&1\\\\\n",
    "x_1^{(1)}&x_1^{(2)} & x_1^{(3)} & .&.&.&x_1^{(m)}\\\\\n",
    ".&. & . &.&.&.& .\\\\\n",
    ".&. & . & .&.&.&.\\\\\n",
    ".&. & . & .&.&.&.\\\\\n",
    "x_n^{(1)}&x_n^{(2)} & x^{(3)} & .&.&.&x_n^{(m)}\\\\\n",
    "\\end{bmatrix}_{(n+1) \\times m}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "luego $h = \\Theta^{T} X $ con dimension $1\\times m$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "La anterior ecuación, es un hiperplano en $\\mathbb{R}^n$. Notese que en caso de tener una sola característica, la ecuación puede ser análizada según lo visto en la sesión de regresion lineal.\n",
    "\n",
    "\n",
    "Para la optimización, vamos a definir la función de coste **$J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n )$** , como la función  asociada a la minima distancia entre dos puntos, según la metrica euclidiana. \n",
    "\n",
    "- Metrica Eculidiana\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n )=\\frac{1}{2m} \\sum_{i=1}^m ( h_{\\Theta} (X)-\\hat{y}^{(i)})^2 =\\frac{1}{2m} \\sum_{i = 1}^m (\\Theta^{T} X - \\hat{y}^{(i)})^2\n",
    "\\end{equation}\n",
    "\n",
    "Otras métricas pueden ser definidas como sigue en la siguiente referencia.  [Metricas](https://jmlb.github.io/flashcards/2018/04/21/list_cost_functions_fo_neuralnets/).\n",
    "\n",
    "Nuestro objetivo será encontrar los valores mínimos \n",
    "$\\Theta = \\theta_0,\\theta_1,\\theta_2,...,\\theta_n$ que minimizan el error, respecto a los valores etiquetados y esperados $\\hat{y}$ \n",
    "\n",
    "\n",
    "Para encontrar $\\Theta$ opmitimo, se necesita  minimizar la función de coste, que permite obtener los valores más cercanos,  esta minimización podrá ser realizada a través de diferentes metodos, el más conocido es el gradiente descendente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5ApulK23HfL"
   },
   "source": [
    "\n",
    "## Gradiente descendente\n",
    "\n",
    "Consideremos la función de coste sin realizar el promedio  de funcion de coste:\n",
    "\\begin{equation}\n",
    "\\Lambda^T =\n",
    "\\begin{bmatrix}\n",
    "(\\theta_0 1 + \\theta_1 x_1^1+\\theta_2 x_2^2 + \\theta_3 x_3^3 +...+ \\theta_n x_n^n - \\hat{y}^{1})^2 \\\\\n",
    "(\\theta_0 1+ \\theta_1 x_1^1+\\theta_2 x_2^2 + \\theta_3 x_3^3 +...+ \\theta_n x_n^n - \\hat{y}^{2})^2\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "(\\theta_0 1 + \\theta_1 x_1^m+\\theta_2 x_2^m + \\theta_3 x_3^m +...+ \\theta_n x_n^m - \\hat{y}^{m})^2\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "$\\Lambda= [\\Lambda_1,\\Lambda_2, ...,\\Lambda_m]$\n",
    "\n",
    "$J = \\frac{1}{2m} \\sum_{i}^m \\Lambda_i $\n",
    "\n",
    "El gradiente descente, puede ser escrito como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\vec{\\Theta} =  - \\alpha \\nabla J(\\theta_0, \\theta_1,...,\\theta_n)\n",
    "\\end{equation}\n",
    "\n",
    "escogiendo el valor j-esimo tenemos que:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_j}\n",
    "\\end{equation}\n",
    "\n",
    "Aplicando lo anterior a a funcion de coste asociada a la metrica ecuclidiana, tenemos que:\n",
    "\n",
    "Para $j = 0$, \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_0 :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_0} = \\frac{1}{m}\\alpha \\sum_{i=1}^m (\\theta_j X_{ji} - \\hat{y}^{(i)}) 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Para $0<j<n $\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_j} = \\frac{1}{m} \\alpha\\sum_{i=1}^m (\\theta_{j} X_{ji} - \\hat{y}^{(i)}) X_j\n",
    "\\end{equation}\n",
    "\n",
    "donde X_j es el vector de entrenamiento j-esimo.\n",
    "\n",
    "Lo  anterior puede ser generalizado como siguem, teniendo presente que $X_0 = \\vec{1}$\n",
    "\n",
    "\n",
    "Para $0\\leq j<n$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j :=  - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1,...\\theta_j...,\\theta_n)}{\\partial \\theta_j} = \\frac{1}{m} \\alpha\\sum_{i=1}^m (\\theta_j X_{ji} - \\hat{y}^{(i)}) X_j \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "# Vectorizando el grandiente descendete, tenemos que:\n",
    "\\begin{equation}\n",
    "\\nabla J = \\Lambda^T X\n",
    "\\end{equation}\n",
    "\n",
    "Luego:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Theta=\\Theta-\\alpha \\nabla J\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eavd6K24VDP"
   },
   "source": [
    "# Laboratorio 05\n",
    "Objetivo: Programar una regresión multivariada\n",
    "\n",
    "\n",
    "1. Para simular un conjunto de características $x_1$ , $x_2$,..., $x_n$ trabajaremos en la primera parte con dos características de datos aleatorios que presentan un plano y mostraremos que los párametros optimizados se corresponden con el valor esperado.\n",
    "\n",
    "- Definir la ecuación  $y = 2.1*x_1 - 3.1*x_2$, y generar números aleatorios que pertenecen al plano. \n",
    "\n",
    "- Realizar un diagrama 3D de los puntos generados aleatoriamente. \n",
    "\n",
    "\n",
    "Nuestro objetivo será encontrar los valores $\\theta_0 = 0, \\theta_1=2.1, \\theta_1=3.1$ que mejor ajustar el plano, empleando cálculos vectorizados. \n",
    "\n",
    "2. Inicializar conjunto de parámetros $\\Theta$ de manera aleatoria.\n",
    "3. Construir la matrix X con dimensiones $(n+1, m)$, m es el numero de datos de entrenamiento y (n) el número de caracteristicas.\n",
    "4. Calcular la función de coste(revise cuidosamente las dimensiones de cada matriz):\n",
    " \n",
    "  - $h = \\Theta^{T} X $\n",
    "  - $\\Lambda= (h -Y) $\n",
    "  - $\\Lambda*= (h -Y)^2 $\n",
    "  - $\\Lambda= [\\Lambda_1,\\Lambda_2, ...,\\Lambda_m]$\n",
    "  - $J = \\frac{1}{2m} \\sum_{i}^m \\Lambda_i $\n",
    "\n",
    "5. Aplicar el gradiente descendente: \n",
    "  - Encontrar el gradiente.\n",
    "    $\\nabla J$ = \\Lambda X.T\n",
    "  \n",
    "  - Actualizar los nuevos parametros: \n",
    "    $\\Theta_{n+1}=\\Theta_{n}-\\alpha\\nabla J$\n",
    "\n",
    "\n",
    "6. Iterar para encontrar los valores $\\Theta$ que se ajustan el plano. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OriGxD33w_f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Laborario_04_reg_multivaraida.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
