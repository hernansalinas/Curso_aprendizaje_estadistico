{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XgxAFhzLlrzT"},"source":["https://github.com/ssanchezgoe/curso_deep_learning_economia/edit/main/NBs_Google_Colab/DL_S15_CNN_Arquitecturas.ipynb"]},{"cell_type":"markdown","metadata":{"id":"F8v_bGYS23_p"},"source":["## **Clasificación con CNN - Arquitectura**\n","\n","Como vimos las redes convolucionales surgen  a partir de trabajos inspirados en el estudio de la corteza cerebral de los animales y cómo estas reaccioban e interactuaban ante los estímulos visuales, es por esto que una de las principales tareas en las que se usan las redes convolucionales es en la clasificación de imagenes. Como vimos en la clase previa la arquitectura típica de una red convolucional es la siguiente:\n","\n","<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/BqlLRkJ.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n","\n","\n","Si bien existen multiples arquitecturas que varían ciertas características, esta es la forma básica que sigue una red convolucional (convolución-pooling-convolución-pooling y así sucesivamente), la cual a la hora de tener una clasificación se definirá simplemente por la capa de salida que tendrá nuestra red densa conectada al final de la parte de convolución, es decir, tendremos de acuerdo a nuestro objetivo la salida correspondiente, ya sea con una activación softmax o sigmoid por ejemplo.\n","\n","Veamos un ejemplo de clasificación a partir de construir una red convolucional para el **MNIST database of handwritten digits** dataset."]},{"cell_type":"code","metadata":{"id":"kfg7GVJBI9ff"},"source":["import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","from tensorflow import keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fk2C0RMq5R1v"},"source":["(X_train,y_train),(X_test,y_test)=keras.datasets.mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JFj1c68s5fmY"},"source":["Veamos la cantidad de elementos que tenemos en cada conjunto\n"]},{"cell_type":"code","metadata":{"id":"f-kE8FjI5cBT"},"source":["print('Datos de entrenamiento:',X_train.shape)\n","print('Datos de test:',X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2KJsacw57MX"},"source":["Vamos a normalizar los datos"]},{"cell_type":"code","metadata":{"id":"BghJ9skR56Wb"},"source":["X_train=X_train/255.0\n","X_test=X_test/255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z9GxCR8S5yfO"},"source":["fig,ax=plt.subplots(2,10,figsize=(15,5))\n","for i,ax in enumerate(ax.flat):\n","  ax.imshow(X_train[i],cmap='binary')\n","  ax.set_axis_off()\n","  ax.set_title(y_train[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3qdmH0P7JtR"},"source":["Como vemos en el dataset tenemos un conjunto de números escritos a mano, ahora construyamos una red convolucional qu me permita hacer predicción de sus etiquetas"]},{"cell_type":"code","metadata":{"id":"3a5bmdDo5ypf"},"source":["keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a259YtV05ysK"},"source":["model=keras.models.Sequential([\n","                               keras.layers.Conv2D(40,kernel_size=3,strides=1,padding='same',activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1)),\n","                               keras.layers.MaxPool2D(pool_size=2),\n","                               keras.layers.Conv2D(80,kernel_size=3,strides=1,padding='valid',activation='relu',kernel_initializer='he_uniform'),\n","                               keras.layers.MaxPool2D(pool_size=2),\n","                               keras.layers.Conv2D(100,kernel_size=3,strides=2,padding='valid',activation='relu',kernel_initializer='he_uniform'),\n","                               keras.layers.MaxPool2D(pool_size=2),\n","                               keras.layers.Flatten(),\n","                               keras.layers.Dense(50,activation='relu',kernel_initializer='he_uniform'),\n","                               keras.layers.Dense(20,activation='relu',kernel_initializer='he_uniform'),\n","                               keras.layers.Dense(10,activation='softmax')\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2Lgajqc5ywA"},"source":["model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAUXNUOx-lNK"},"source":["X_train=X_train.reshape(60000,28,28,1)\n","X_test=X_test.reshape(10000,28,28,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1aJIwr6-U66"},"source":["history=model.fit(X_train,y_train,epochs=10,validation_split=0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEuUWGdm_w51"},"source":["model.evaluate(X_test,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_hQA1a94AQ_L"},"source":["Como anotación, con las redes convolucionales podemos realizar no solo clasificación, sino además tareas como la de localización, donde queremos predecir una caja como frontera alrededor de un objeto, donde esta vez no solo debemos predecir la clasificación del objeto sino que además debemos obtener las coordenadas de la caja en forma de frontera.\n","\n","\n","<p><img alt=\"Colaboratory logo\" height=\"300px\" src=\"https://i.imgur.com/nxg6gy9.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>\n","\n","Donde quizás lo mas complicado es tener un dataset con estas etiquetas, por lo tanto una parte ardua de la detección es lograr etiquetar nuestro conjunto de imagenes con las fronteras correspondientes, para esto se pueden usar herramientas de etiquetado como VGG Image Annotator, LabelImg, OpenLabeler or ImgLab. [link](https://arxiv.org/pdf/1611.02145.pdf)"]},{"cell_type":"markdown","metadata":{"id":"9F7mYDLVXtTm"},"source":["# Ejercicio usando Cifar100"]},{"cell_type":"markdown","metadata":{"id":"_2WUHjSKYirm"},"source":["Importemos algunas librerias que seran de utilidad"]},{"cell_type":"code","metadata":{"id":"HPHPQauaXwYB"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","from tensorflow import keras "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYydqHoDYmWC"},"source":["importemos los datos usando keras"]},{"cell_type":"code","metadata":{"id":"I3mI8SIRXwYH"},"source":["(train_image, train_label) , (test_image, test_label) = keras.datasets.cifar100.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ug8MJ0lXwYJ"},"source":["test_image.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Asr8STOXXwYM"},"source":["train_image.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbDhc6iNXwYQ"},"source":["plt.imshow(train_image[1])\n","plt.grid(0)\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lO0N35waYpSm"},"source":["veamos algunas de las figuras que contiene el dataset"]},{"cell_type":"code","metadata":{"id":"9Drd4A52XwYS"},"source":["fig , ax = plt.subplots(3,7, figsize=(15,7))\n","for i, ax in enumerate(ax.flat):\n","  ax.imshow(train_image[i], cmap='Greys')\n","  ax.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bOlauJFWYszL"},"source":["escalemos los datos en un rango de 0 a 1"]},{"cell_type":"code","metadata":{"id":"-jdYZIMWXwYb"},"source":["train_image = train_image / 255.0\n","test_image = test_image / 255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5c5diUEOXwYe"},"source":["keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0TVxjQsYwwq"},"source":["creemos nuestro modelo de CNN usando keras"]},{"cell_type":"code","metadata":{"id":"J-J4P9z5XwYg"},"source":["model = keras.models.Sequential([\n","      keras.layers.Conv2D(32, kernel_size=3, activation='elu', kernel_initializer='he_normal' , padding='same',strides=(1,1) ,input_shape=(32, 32, 3)),\n","      keras.layers.BatchNormalization(),\n","      keras.layers.Conv2D(32, kernel_size=3, activation='elu', kernel_initializer='he_normal' , padding='same',strides=(1,1) ,input_shape=(32, 32, 3)),\n","      keras.layers.BatchNormalization(),\n","      keras.layers.MaxPool2D(pool_size=(2,2)),\n","      keras.layers.Dropout(0.2),\n","\n","      keras.layers.Conv2D(64, kernel_size=3, activation='elu', kernel_initializer='he_normal' , padding='same', strides=(1,1)),\n","      keras.layers.BatchNormalization(),\n","      keras.layers.Conv2D(64, kernel_size=3, activation='elu', kernel_initializer='he_normal' , padding='same', strides=(1,1)),\n","      keras.layers.BatchNormalization(),\n","      keras.layers.MaxPool2D(pool_size=(2,2)),\n","      keras.layers.Dropout(0.2),\n","\n","      keras.layers.Conv2D(128, kernel_size=3, activation='elu', kernel_initializer='he_normal' , padding='same', strides=(1,1)),\n","      keras.layers.BatchNormalization(),\n","      keras.layers.Conv2D(128, kernel_size=3, activation='elu', kernel_initializer='he_normal',  padding='same', strides=(1,1)),\n","      keras.layers.BatchNormalization(),\n","      keras.layers.MaxPool2D(pool_size=(2,2)),\n","      keras.layers.Dropout(0.2),\n","\n","      keras.layers.Flatten(),\n","      keras.layers.Dense(128, activation='elu', kernel_initializer='he_normal'),\n","\n","      keras.layers.Dropout(0.2),\n","\n","      keras.layers.Dense(100, activation='softmax')\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-Bnkig1XwYi"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"niNAjVfcXwYk"},"source":["model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wD1119YMY4Cq"},"source":["usemos early stopping en nuestro modelo "]},{"cell_type":"code","metadata":{"id":"WqjF-fbYXwYm"},"source":["early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85WTOhuhY6MW"},"source":["procedamos a entrenar nuestro modelo"]},{"cell_type":"code","metadata":{"id":"6IFb-n6yXwYo"},"source":["history = model.fit(train_image, train_label, epochs=30, validation_split=0.2 , batch_size=64, callbacks=[early_stopping])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pr8dca1uYGi9"},"source":["model.evaluate(test_image, test_label, verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KNznY0xPsyAL"},"source":["# Arquitecturas de CNNs (más complejas)"]},{"cell_type":"markdown","metadata":{"id":"scf3rCsCPBik"},"source":["A lo largo de los años, se han desarrollado variantes la arquitectura fundamental que de una CNN que vimos en la sesión pasar. Dichas arquidecturas nuevas han contrubuido en el avance de este campo. Una buena medida del progreso es la tasa de error obtenidas en competencias como el desafío ILSVRC ImageNet. En esta competencia, la tasa de error entre las 5 mejores arquitecturas para la clasificación de imágenes cayó del 26% a menos del 2,3% en solo seis años. A continuación se muestra la tabla de arquitecturas con mayor presición, o menor tasa de error:\n","\n","<p><img alt=\"Colaboratory logo\" height=\"430px\" src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/competition.png?raw=true\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> \n","\n","\n","En lo que respecta a nosotros estudiaremos la arquitectura de dos CNN de ellas, dada su importacia:\n","\n","\n","1. VGGNet16\n","\n","2. ResNet\n","\n","Debido al costo computacional tan elevado que representa entrenar dichas redes, Keras lo hizo ya por nosotros!. Importaremos el pesos preentrenados de estas dos arquitecturas. Para mayor información consultar el siguiente [link](https://keras.io/applications/)"]},{"cell_type":"markdown","metadata":{"id":"X8RSiF5oJ2zx"},"source":["**VGGNet**\n","\n","El segundo lugar en el desafío ILSVRC 2014 lo obtubo VGGNet14, desarrollado por K. Simonyan y A. Zisserman. Tenía una arquitectura muy simple y clásica, con 2 o 3 capas convolucionales, una capa de agrupación, luego nuevamente 2 o 3 capas convolucionales, una capa de agrupación, etc. (con un total de solo 16 capas convolucionales), más una red densa al final con 2 capas ocultas y la capa de salida. Solo usó 3 × 3 filtros, pero muchos filtros.\n","\n","<p><img alt=\"Colaboratory logo\" height=\"400px\" src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/VGG16-Architecture-with-Softmax-layer-replaced-used-as-the-base-model-for-each-classifier.png?raw=true\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p> "]},{"cell_type":"code","metadata":{"id":"exFjudD5J2Up"},"source":["from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","\n","# Indian elephant\n","!wget https://upload.wikimedia.org/wikipedia/commons/f/f9/Zoorashia_elephant.jpg -O indian_elephant.jpg\n","# African elephant\n","!wget https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/african_elephant_bull.jpg?raw=true -O african_elephant.jpg\n","\n","# Choose the elephant to be classified\n","img_path = 'african_elephant.jpg'\n","\n","img = mpimg.imread(img_path)\n","implot = plt.imshow(img)\n","\n","print(\"Tamaño de la imagen:\",img.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuvT0dCQPOP3"},"source":["from keras.preprocessing import image\n","\n","model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcZVhdOSxShr"},"source":["img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = keras.applications.vgg16.preprocess_input(x)\n","\n","features = model.predict(x)\n","\n","print(\"Prediction\", keras.applications.vgg16.decode_predictions(features, top=3)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElSkRj0uJTrC"},"source":["**ResNet**\n","\n","El desafío de ILSVRC 2015 fue ganado mediante una Red Residual (o ResNet), desarrollada por Kaiming He et al., lográndose una tasa de error inferior a 3.6%, usando una CNN muy profunda compuesta de 152 capas. La clave para poder entrenar una red tan profunda es usar skip connections (también llamadas conexiones de atajo): la señal que alimenta a una capa también se agrega a la salida de una capa ubicada un poco más arriba en la pila. \n","\n","Lo anterior resulta útil ya que, al entrenar una red neuronal, el objetivo es hacer que modele una función objetivo $h(x)$. Si agrega la entrada $x$ a la salida de la red (es decir, agrega una skip connection), la red se verá obligada a modelar $f(x) = h (x) - x$ en lugar de $h(x)$. Esto se llama aprendizaje residual (ver Figura 14-15).\n","\n","<p><img alt=\"Colaboratory logo\" height=\"350px\" src=\"https://github.com/ssanchezgoe/diplomado_udea/blob/master/image/resnet.png?raw=true\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>"]},{"cell_type":"code","metadata":{"id":"hlJMtkVdQKbm"},"source":["keras.backend.clear_session()\n","\n","from keras.applications.resnet import ResNet50\n","from keras.applications.resnet import preprocess_input, decode_predictions\n","import numpy as np\n","\n","model = keras.applications.resnet50.ResNet50(weights='imagenet')\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQn3dtyxQpRn"},"source":["img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = keras.applications.resnet50.preprocess_input(x)\n","\n","preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","print('Predicted:', keras.applications.resnet50.decode_predictions(preds, top=3)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AqAcgKwmKcCT"},"source":["# Transfer learning (transferencia de aprendizaje)\n","Como hemos visto las DNN deben ser entrenadas en varias épocas, usando algoritmos como el de backpropagation para actualizar los pesos y poco a poco reducir la perdida. Dicho proceso puede consumir una gran cantidad de recursos, monetarios, temporales o computacionales.\n","\n","Por ello se ha implementado una técnica llamada transferencia de aprendizaje, en la cuál se usan los pesos entrenados de alguna red que realice una tarea parecida a aquella que queremos resolver. Así no se realizará un entrenamiento partiendo de pesos inicializados de manera aleatoria, si no de pesos que ya han creado filtros adecuados para la tarea dada.\n","\n","Con un modelo preentrenado podemos hacer tres cosas:\n","\n","\n","*   Usarlo como nos es entregado para la tarea que deseamos (por ejemplo usar VGG como un clasificador de imagenes con 1000 clases)\n","*   Usar sus pesos (o parte de ellos) como extractor de características previo al uso de nuestra propia DNN. (Los pesos del modelo prenetrenado no se van a actualizar en posteriores etápas de entrenamiento o fine tuning)\n","*    Usar los pesos (o parte de ellos) como parte de nuestra red neuronal, puediendo actualizar o no alguna de las capas en posteriores etápas de entrenamiento o fine tuining.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h11tLPYXRC3c"},"source":["### ¿Por qué funciona el transfer learning?\n","Para explicar porqué funciona ésta estrategia debemos recordar como funcionan las redes convolucionales (aunque la tranferencia de aprendizaje puede usarse para cualquier arquitectura).\n","\n","![CNN](https://miro.medium.com/max/2344/1*3aT9KWCeQ6wIYdLLhD4mCw.png)\n","\n","Las capas convolucionales irán creando poco a poco mapas de características que minimicen la función de perdida, dichos mapas, como ya dijimos, serán más simples en las capas iniciales (generando kernels que detectan colores o lineas básicas) y a medida que la red se hace más profunda dichas características se irán mezclando y creando kernels más complejos (detectando rostros completos, ojos u otras características complejas de nuestros datos).\n"]},{"cell_type":"markdown","metadata":{"id":"-VkSyQi6T6lv"},"source":["Veamos por ejemplo en las siguientes imagenes los filtros y las partes de las imágenes qué más activan dichos filtros:\n","![alt text](https://miro.medium.com/max/907/1*jPCEik198_CjtmSL2H6o4g.png)\n","![alt text](https://miro.medium.com/max/818/1*1Y6HZxK-lOmqB8KnizTCow.png)\n","\n","Note como en las primeras capas el modelo reconoce lineas simples y colores, y en las últimas  los filtros se han complejizado suficiente como para activar rostros completos."]},{"cell_type":"markdown","metadata":{"id":"hmDPRI1yUitk"},"source":["De éste ejemplo es claro que, si tenemos problemas relativamente parecidos o si los filtros aprendidos por nuestra red son útiles en otros problemas será mejor inicializar nuestros pesos con dichos filtros y no de una forma aleatoria.\n","Por ejemplo, si nuestra tarea es clasificar gatos y perros, será mejor iniciar con dicha red (que ya ha creado filtros para ellos) que hacerlo desde cero, y, de ser necesario, hacer un fine tuning de la red con nuevos ejemplos."]},{"cell_type":"markdown","metadata":{"id":"Juc3g3pqVRuU"},"source":["En keras tenemos algunas redes preentrenadas, todas ellas bastante buenas y de uso libre. [Ver acá algunos modelos preentrenados en Keras](https://keras.io/applications/)"]},{"cell_type":"markdown","metadata":{"id":"xGt8HvLPV1-p"},"source":["Hagamos un ejemplo del uso de VGG16 en keras para hacer transferencia de aprendizaje"]},{"cell_type":"code","metadata":{"id":"rgUJxFfQUKft"},"source":["%tensorflow_version 2.x\n","from tensorflow.keras.applications.vgg16 import VGG16\n","model = VGG16()\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fO4ZFRk4WbAa"},"source":["Hemos cargado el modelo VGG16, claramente más complejo que aquellos que hemos usado hasta ahora y sin embargo el más simple en nuestra colección de modelos preentrenados.\n","\n","Veamos cómo usarlo."]},{"cell_type":"code","metadata":{"id":"YoFvZ4iXWKu3"},"source":["!wget 'https://pbs.twimg.com/profile_images/973248614469312512/ffd7wIOW_400x400.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3hb9zMTXDMp"},"source":["from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.applications.vgg16 import decode_predictions\n","from tensorflow.keras.applications.vgg16 import VGG16\n","import matplotlib.pyplot as plt\n","#carguemos la imagen y preprocesemosla\n","image = load_img('ffd7wIOW_400x400.jpg', target_size=(224, 224)) #VGG necesita imagenes de 224x224 pixeles\n","plt.imshow(image)\n","plt.show()\n","image = img_to_array(image)\n","\n","#procesamos la imagen con el mismo preprocesado que se aplicó a las imagenes de entrenamiento\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","image = preprocess_input(image)\n","#cargamos el modelo y predecimos las probabilidades\n","model = VGG16()\n","yhat = model.predict(image)\n","label = decode_predictions(yhat)\n","label = label[0][0]\n","\n","print('%s (%.2f%%)' % (label[1], label[2]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgoRpXKRYBnc"},"source":["En éste ejemplo, usando la red como clasificador vemos que nos dice que la imagen es de un gato y además que es un gato de raza atigrado (tabby) con una certeza de 31.8% (el resto de la probabilidad se reparte entre las otras 999 clases)"]},{"cell_type":"markdown","metadata":{"id":"zOCx5BJrYyMB"},"source":["Usemos ahora VGG como un preprocesador para hacer extracción de características.\n","\n","En éstos casos se saca provecho del hecho de que cada capa está extrayendo información de ciertas características de la imagen, podemos usar las salidas de dichas capas como entradas para nuestros propios modelos, usandolos de manera efectiva como estractores de features.\n","\n","En éste caso usaremos la pa penúltima capa fully conected con 4096 neuronas como salida, es decir \"mocharemos\" las capas posteriores a dicha capa de la arquitectura, y ésta salida alimentará nuestra pequeña red neuronal."]},{"cell_type":"code","metadata":{"id":"VH8b1zNcXl23"},"source":["%tensorflow_version 1.x\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","# cargamos VGG sin las últimas capas\n","model = VGG16(include_top=False, input_shape=(300, 300, 3))\n","# agreguemos capas nuevas\n","flat1 = Flatten()(model.outputs)\n","class1 = Dense(1024, activation='relu')(flat1)\n","output = Dense(10, activation='softmax')(class1)\n","#nuevo modelo\n","model = Model(inputs=model.inputs, outputs=output)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QF8Ud87_dBAv"},"source":["Note que en el ejemplo anterior hemos definido el modelo no de forma secuencial si no funcional.\n","\n","Ahora si lo que queremos es que algunas de las capas no se entrenen (congelar los pesos), debemos poner la bandera \"trainable =False\"."]},{"cell_type":"code","metadata":{"id":"PhjL9drHbDrN"},"source":["model = VGG16(include_top=False, input_shape=(300, 300, 3))\n","for layer in model.layers:\n","\tlayer.trainable = False\n","%tensorflow_version 2.x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOD1tjrcdne2"},"source":["Acá podemos congelar cuantas capaz deseemos y posteriormente entrenar las demás.\n","\n","El uso de la transferencia de aprendizaje nos ayuda a mejorar los tiempos de entrenamiento, a mejorar las capacidades predictivas y a realizar nuevas tareas aprovechando los modelos y procesos de entenamiento realizados anteriormente (construir sobre lo construído)."]},{"cell_type":"markdown","metadata":{"id":"0jWzAFTjbK2Q"},"source":["# Ejercicio: Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"4ZCfIFNsjagx"},"source":["Importar las librerías y los datos"]},{"cell_type":"code","metadata":{"id":"dNyQQQONbJnt"},"source":["from matplotlib import pyplot as plt\n","from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications import VGG16\n","from keras.datasets import cifar100\n","\n","(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n","plt.imshow(x_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_UTLoxSBjgPg"},"source":["Preprocesar los datos de acuerdo a los parámetros de VGG16"]},{"cell_type":"code","metadata":{"id":"BDeaq3HdhkF4"},"source":["x_train=preprocess_input(x_train)\n","x_test=preprocess_input(x_test)\n","plt.imshow(x_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnw2nw0ajkto"},"source":["Cargar la base convolucional de VGG16 (Sin la parte densa)"]},{"cell_type":"code","metadata":{"id":"iYl8Wh3JbPQg"},"source":["conv_base = VGG16(weights='imagenet',include_top=False,input_shape=(32, 32, 3))\n","conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G3kV4Wu0jrP0"},"source":["Congelar las capas convolucionales excepto las últimas 4"]},{"cell_type":"code","metadata":{"id":"qk5mlfg_bRlU"},"source":["conv_base.trainable = True\n","for layer in conv_base.layers[:-4]:\n","    layer.trainable = False\n","for layer in conv_base.layers:\n","    print(layer, layer.trainable)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xx1s_v5lju2t"},"source":["Crear el resto del modelo, agregando las capas densas"]},{"cell_type":"code","metadata":{"id":"YE70oylObdVp"},"source":["model = models.Sequential()\n","model.add(conv_base)\n","model.add(layers.Flatten())\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.BatchNormalization())\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(100, activation='softmax'))\n","\n","model.compile('adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dYAcR_CbfPz"},"source":["model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXxdEVPxbuZ_"},"source":[],"execution_count":null,"outputs":[]}]}